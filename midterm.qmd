---
title: "EVALUACION MIDTERM - Modelamiento de balance de masa"
author: "Ruben Basantes" 
date: "2025-10-04"
format:
  html:
    embed-resources: true
execute:
  kernel: "Python (.venv)"
---

## ¿Qué es la regresión lineal simple?

Es un método estadístico que se enfoca en analizar la relación lineal entre dos variables:

- Una **variable independiente** X (por ejemplo: horas de estudio)
- Una **variable dependiente** Y 

El modelo tiene esta forma:

Y = b0 + b1*X

Donde:

- b0: es la **intersección (intercepto)**. Es el valor de Y cuando X = 0.  
- b1: es la **pendiente de la recta**, y representa cuánto cambia Y por cada unidad que aumenta X.

Queremos encontrar una recta que se ajuste lo mejor posible a nuestros datos, es decir que 
exista una minima diferencia entre el valor real y lo predicho. Este es el criterio de **mínimos cuadrados**.

## 0. Importando librerias
Se importan las librerias desde los paquetes
```{python}
import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import learning_curve
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
```

## 1. Carga y exploración inicial del dataset.
Se cargan los datos del archivo .csv, que proviene de una cobertura de puntos en formato shapefile.La tabla contiene atributos asociados a los balanves de masa.
```{python}
df = gpd.read_file("ant_20222016_Dh_adjslp_pts.dbf")
# Ver primeras filas
df.head()
```

Se identifican las variables con valores faltantes y se visualizan en un gráfico de barras.
sto permite decidir si imputar valores o eliminar registros incompletos.
```{python}
# Porcentaje de valores faltantes por columna
nan_percent = df.isna().mean() * 100
nan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)

print("Porcentaje de valores faltantes por columna:")
print(nan_percent_sorted)

# === Visualización opcional con gráfico de barras ===
nan_percent_sorted[nan_percent_sorted > 0].plot(
    kind='barh', color='salmon', figsize=(8,5)
)
plt.title("Porcentaje de valores faltantes por variable")
plt.xlabel("Porcentaje (%)")
plt.ylabel("Variable")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()
```

La seleccion de seleccion de variables considera:
VALUE: variable dependiente (0 = sin deslizamiento, 1 = con deslizamiento)
features: variables explicativas derivadas del terreno o clima
Se eliminan filas con valores faltantes.
```{python}
# La tercera columna ('desliz_occ') es la variable dependiente (0 = no, 1 = sí)
target_col = 'dhdt'

# Seleccion de variables explicativas numéricas
# Excluimos 'slp1','tri1', 'elev1'
# Puedes ajustar según tus columnas reales
features = [
    'asp1', 'rgh1',  
    'tpi1', 'prec1', #'tmed1',
]

# Eliminacion filas con NaN
df_clean = df.dropna(subset=features + [target_col])
df_clean
```

```{python}
# Crear subconjunto de datos
X = df_clean[features]
y = df_clean[target_col].astype(float)
# Verificar valores faltantes
X.info()
X.describe()
```

La matriz de correlacion permite identificar colinealidad entre variables explicativas.
Valores altos (>0.8 o <−0.8) pueden indicar redundancia entre variables.
```{python}
# --- Asegurar que solo se usen variables numéricas ---
correlation_matrix = X.corr()
print(correlation_matrix)

# Visualizar matriz de correlación
plt.figure(figsize=(7, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Matriz de Correlación de los predictores')
plt.show()
```

## 2. División en conjuntos de entrenamiento y prueba (train_test_split).
Se dividen los datos:
 - 80% para entrenar el modelo
 - 20% para evaluar su desempeño
```{python}
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Tamaño entrenamiento:", X_train.shape)
print("Tamaño prueba:", X_test.shape)
```

## 3. Definición y entrenamiento del modelo utilizando Pipeline.
Etapas del pipeline:

 - Imputer: reemplaza valores faltantes por la media.
 - Scaler: estandariza las variables (media = 0, desviación = 1).
 - Regresión logística: modelo lineal que estima la probabilidad de deslizamiento.
 - class_weight='balanced': ajusta el peso de las clases para evitar sesgo hacia la clase mayoritaria.
```{python}
# Definir pipeline: estandarización + regresión logística
#pipe = Pipeline([
#    ('imputer', SimpleImputer(strategy='mean')),  # or 'median'
#    ('scaler', StandardScaler()),
#    ('logreg', LogisticRegression(solver='liblinear', class_weight='balanced',max_iter=1000))
#])

pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ("scaler", StandardScaler()),
    ("regressor", LinearRegression())
])

# Entrenar modelo
pipe.fit(X_train, y_train)
```

## 4. Generación de predicciones.
```{python}
y_pred = pipe.predict(X_test)
y_pred[:5]
#y_prob = pipe.predict_proba(X_test)[:, 1]
```

## 5. Evaluación del modelo con métricas apropiadas.
```{python}
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
```

El *Mean Squared Error (MSE)* mide cuánto se desvían en promedio las predicciones de los valores reales. Un MSE de 0.22 significa que el error cuadrático medio entre los valores reales del balance de masa y los predichos por el modelo es aproximadamente 0.22 metros, siendo muy alto y poco eficiente. Entonces el modelo no está logrando capturar bien la relación entre las variables explicativas y la variable dependiente. 
```{python}
print("Mean Squared Error (MSE):", mse)
```

El *R²* mide la proporción de la variabilidad de la variable objetivo que logra explicar el modelo. Un R² = 0.14 india que el modelo solo explica aproximadamente el 14% de la variación de los datos. Asi, las variables explicativas actuales (aspecto, pendiente, elevación, rugosidad, etc.) no son suficientes para explicar el comportamiento del balance de masa.
```{python}
print("R² Score:", r2)
```

## 6. Visualizaciones e interpretación de resultados.
En la matriz de comfusion
Diagonal principal → predicciones correctas.
Fuera de la diagonal → errores del modelo.
```{python}
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel("Valores Reales")
plt.ylabel("Valores Predichos")
plt.title("Regresión Lineal: Valores Reales vs Predichos")
plt.show()
```

